import gspread
import json
import time
import numpy as np
import openai
import faiss
from oauth2client.service_account import ServiceAccountCredentials
from dotenv import load_dotenv
import os

# .envèª­ã¿è¾¼ã¿ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œæ™‚ï¼‰
if os.getenv("GITHUB_ACTIONS") != "true":
    load_dotenv()

# OpenAI APIã‚­ãƒ¼è¨­å®š
openai.api_key = os.getenv("OPENAI_API_KEY")
if not openai.api_key:
    raise ValueError("OPENAI_API_KEY is not set.")

# èªè¨¼è¨­å®š
scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
credentials = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)
gc = gspread.authorize(credentials)

# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿
spreadsheet = gc.open_by_key("1ApH-A58jUCZSKwTBAyuPZlZTNsv_2RwKGSqZNyaHHfk")
sheet = spreadsheet.worksheet("knowledge")

records = sheet.get_all_records()
knowledge = {row['title']: [row['content']] for row in records}

# ä¿å­˜
os.makedirs("data", exist_ok=True)
with open("data/knowledge.json", "w", encoding="utf-8") as f:
    json.dump(knowledge, f, ensure_ascii=False, indent=2)

print("âœ… data/knowledge.json ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")

# ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿å‡¦ç†
texts = [f"{title}ï¼š{content[0]}" for title, content in knowledge.items()]
EMBED_MODEL = "text-embedding-3-small"
BATCH_SIZE = 100

def get_embeddings_batch(text_batch, retries=3, delay=3):
    for attempt in range(retries):
        try:
            response = openai.embeddings.create(model=EMBED_MODEL, input=text_batch)
            return [np.array(d.embedding, dtype="float32") for d in response.data]
        except openai.error.OpenAIError as e:
            print(f"âš ï¸ API error: {e}. Retrying in {delay} sec...")
            time.sleep(delay)
    raise RuntimeError("âŒ Failed to get embeddings after multiple retries.")

print("ğŸ”„ ãƒ™ã‚¯ãƒˆãƒ«ã‚’å†ç”Ÿæˆã—ã¦ã„ã¾ã™...")

all_vectors = []
for i in range(0, len(texts), BATCH_SIZE):
    batch = texts[i:i+BATCH_SIZE]
    vectors = get_embeddings_batch(batch)
    all_vectors.extend(vectors)
    print(f"âœ… Processed {i + len(batch)}/{len(texts)}")

vector_data = np.array(all_vectors, dtype="float32")

# FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆãƒ»ä¿å­˜
dimension = vector_data.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(vector_data)

np.save("data/vector_data.npy", vector_data)
faiss.write_index(index, "data/index.faiss")

print("âœ… ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã¨FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
